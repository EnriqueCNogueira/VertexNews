Defini√ß√£o do Projeto: Pipeline Inteligente de Not√≠cias de Marketing

Este projeto visa construir um sistema automatizado (um "pipeline") capaz de coletar, processar, analisar e apresentar not√≠cias relevantes do setor de marketing digital. A finalidade principal √© transformar um grande volume de informa√ß√µes dispersas na web em insights valiosos, facilitando a cria√ß√£o de conte√∫do estrat√©gico para redes sociais, como o Instagram.

Etapas principais do Projeto:

‚úÖ 1. Configura√ß√£o inicial: Preparar o ambiente e importar as ferramentas (bibliotecas Python) necess√°rias para todo o processo.
   - Ambiente virtual criado
   - Depend√™ncias instaladas (requirements.txt)
   - Estrutura de m√≥dulos organizada

‚úÖ 2. Coleta de dados: Obter links e metadados b√°sicos de not√≠cias de diversas fontes web usando web scraping (Requests + BeautifulSoup).
   - Scrapers implementados para:
     * GKPB (completo - inclui extra√ß√£o de imagens)
     * Exame (b√°sico - falta extra√ß√£o de imagens)
     * Meio & Mensagem (b√°sico - falta extra√ß√£o de imagens)
     * Mundo do Marketing (b√°sico - falta extra√ß√£o de imagens)

‚úÖ 3. Extra√ß√£o de texto: Acessar cada link coletado para extrair o conte√∫do completo do artigo (BeautifulSoup).
   - Sistema de extra√ß√£o implementado com seletores espec√≠ficos por fonte
   - Cache em mem√≥ria para textos completos

‚úÖ 4. Armazenamento (sqlite): Salvar todos os dados coletados e extra√≠dos em um banco de dados local para acesso e gest√£o eficientes.
   - Arquitetura de dois bancos implementada:
     * Banco auxiliar (noticias_aux.db) - tempor√°rio para processamento
     * Banco principal (noticias.db) - persistente para API
   - Sistema de opera√ß√µes CRUD completo
   - Cache inteligente em mem√≥ria

‚úÖ 5. Sumariza√ß√£o: Gerar resumos curtos dos textos completos usando um modelo de IA (biblioteca Transformers).
   - Modelo unicamp-dl/ptt5-small-portuguese-vocab implementado
   - Sistema de sumariza√ß√£o com tratamento de erros
   - Otimiza√ß√£o para CPU/GPU

‚úÖ 6. Clusteriza√ß√£o: Agrupar not√≠cias com temas semelhantes transformando resumos em vetores (TF-IDF) e aplicando K-Means.
   - Vetoriza√ß√£o TF-IDF implementada
   - Algoritmo K-Means configur√°vel
   - Sistema de interpreta√ß√£o de clusters

‚úÖ 7. Interpreta√ß√£o de clusters: Analisar os grupos formados para identificar os temas principais e atribuir r√≥tulos descritivos.
   - Extra√ß√£o de palavras-chave por cluster
   - Sistema de pontua√ß√£o de relev√¢ncia
   - An√°lise estat√≠stica dos grupos

‚úÖ 8. Sele√ß√£o relevante: Pontuar e selecionar as 15 not√≠cias mais estrat√©gicas para conte√∫do, baseando-se em crit√©rios definidos (palavras-chave, marcas, etc.).
   - Sistema de pontua√ß√£o baseado em:
     * Marcas grandes (peso 5)
     * Campanhas e a√ß√µes (peso 3)
     * Palavras de impacto (peso 2)
   - Transfer√™ncia autom√°tica para banco principal
   - Limpeza do banco auxiliar

üîÑ 9. Cria√ß√£o da API: Construir uma interface para acessar os dados processados do banco de dados (Framework web como Flask).
   - STATUS: Planejado mas n√£o implementado
   - Endpoints necess√°rios:
     * GET /api/news - not√≠cias mais recentes
     * GET /api/news/search - busca por termo
     * GET /api/news/stats - estat√≠sticas
     * GET /api/news/{id} - not√≠cia espec√≠fica
     * GET /api/health - sa√∫de da API

üîÑ 10. Cria√ß√£o do site: Desenvolver uma interface visual para exibir as not√≠cias, resumos, temas e as 15 mais relevantes (HTML/CSS/JS ou framework frontend).
    - STATUS: N√£o implementado
    - Interface necess√°ria para:
      * Visualiza√ß√£o das not√≠cias selecionadas
      * Exibi√ß√£o dos clusters e temas
      * Busca e filtros
      * Dashboard com estat√≠sticas

üîÑ 11. Melhorias pendentes:
    - Extra√ß√£o de imagens para sites Exame, Meio & Mensagem e Mundo do Marketing
    - Implementa√ß√£o da API REST
    - Desenvolvimento do frontend
    - Testes automatizados
    - Documenta√ß√£o da API

üîÑ 12. Finish task: Apresentar o pipeline completo, a API e o site funcionando.
    - STATUS: Em desenvolvimento
    - Pipeline de processamento: ‚úÖ Completo
    - API REST: ‚ùå Pendente
    - Site frontend: ‚ùå Pendente

Cada uma dessas etapas constr√≥i sobre a anterior, transformando raw data (dados brutos) da web em insights acion√°veis para a cria√ß√£o de conte√∫do estrat√©gico.

ARQUITETURA ATUAL:
- Pipeline de processamento: 100% funcional
- Banco de dados: Sistema completo com dois bancos
- Cache inteligente: Implementado
- Tratamento de erros: Sistema robusto
- Logs e monitoramento: Implementado

PR√ìXIMOS PASSOS:
1. Implementar API REST com Flask
2. Desenvolver interface web frontend
3. Melhorar extra√ß√£o de imagens nos scrapers
4. Adicionar testes automatizados
5. Documentar API e deploy